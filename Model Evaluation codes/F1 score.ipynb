{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b86a5a27-92ff-4201-b966-78f4c07c930b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17426fbf-1c1f-4c42-bc9c-c1f11551430d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_f1_score(pred_tokens, true_tokens):\n",
    "    # Create Counter objects for both sets of tokens\n",
    "    pred_counts = Counter(pred_tokens)\n",
    "    true_counts = Counter(true_tokens)\n",
    "    \n",
    "    # Compute the number of common tokens between prediction and truth\n",
    "    common = pred_counts & true_counts\n",
    "    num_same = sum(common.values())\n",
    "    \n",
    "    if num_same == 0:\n",
    "        return 0\n",
    "    \n",
    "    # Precision: proportion of predicted tokens that are correct\n",
    "    precision = num_same / len(pred_tokens)\n",
    "    # Recall: proportion of true tokens that are predicted\n",
    "    recall = num_same / len(true_tokens)\n",
    "    \n",
    "    # F1 score: harmonic mean of precision and recall\n",
    "    f1_score = 2 * (precision * recall) / (precision + recall)\n",
    "    return f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0901aaec-6b29-459e-9637-2a55bb8648e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CSV file\n",
    "data = pd.read_csv(\"Data/q_and_a.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dabc4736-7503-4f8b-9de2-1222dc205e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to apply F1 score calculation to each row\n",
    "def apply_f1_scores(row):\n",
    "    reference_answer = row['reference_answer']\n",
    "    answers = {\n",
    "        'chat_gpt_answer': row['chat_gpt_answer'],\n",
    "        'google_gemini_answer': row['google_gemini_answer'],\n",
    "        'fine_tuned_model_answer': row['fine_tuned_model_answer']\n",
    "    }\n",
    "    \n",
    "    # Tokenize the reference answer\n",
    "    true_tokens = word_tokenize(reference_answer)\n",
    "    \n",
    "    # Dictionary to store F1 scores\n",
    "    f1_scores = {}\n",
    "    \n",
    "    # Calculate F1 score for each model's answer\n",
    "    for model, answer in answers.items():\n",
    "        pred_tokens = word_tokenize(answer)\n",
    "        f1_scores[f'{model}_f1'] = compute_f1_score(pred_tokens, true_tokens)\n",
    "    \n",
    "    return pd.Series(f1_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41ee5e16-cd32-48dc-9b10-53fb98117129",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the function to each row in the dataframe\n",
    "f1_results = data.apply(apply_f1_scores, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36a76641-c15c-4b99-999d-f8d00ec030ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine the original data with the F1 results\n",
    "final_results = pd.concat([data, f1_results], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25561ae9-dd14-4270-a638-30ac6bdd76db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save to a new CSV file\n",
    "final_results.to_csv('Data/q_and_a_F1.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
